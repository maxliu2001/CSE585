{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acca59f6-e89d-4db7-a837-755e0f638054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"garage-bAInd/Open-Platypus\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd4754e-2d43-4266-9a93-3d9a1cbce4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 19940\n",
      "Validation set size: 4986\n"
     ]
    }
   ],
   "source": [
    "train_val_split = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "# Extract the training and validation datasets\n",
    "train_dataset = train_val_split['train']\n",
    "val_dataset = train_val_split['test']\n",
    "\n",
    "# Print the size of the datasets\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3585f4a9-733c-4cd0-9663-3eaf7c0c7f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '', 'output': 'To find the probability of the spinner landing on $C$, I need to subtract the probabilities of the spinner landing on $A$ and $B$ from $1$, since the sum of the probabilities of all possible outcomes is $1$. I can write this as an equation: $P(C) = 1 - P(A) - P(B)$. I know that $P(A) = \\\\frac{1}{3}$ and $P(B) = \\\\frac{5}{12}$, so I can plug those values into the equation and simplify. I get: $P(C) = 1 - \\\\frac{1}{3} - \\\\frac{5}{12} = \\\\frac{12}{12} - \\\\frac{4}{12} - \\\\frac{5}{12} = \\\\frac{3}{12}$. I can reduce this fraction by dividing the numerator and denominator by $3$, and I get: $P(C) = \\\\frac{1}{4}$. ', 'instruction': 'A board game spinner is divided into three parts labeled $A$, $B$  and $C$. The probability of the spinner landing on $A$ is $\\\\frac{1}{3}$ and the probability of the spinner landing on $B$ is $\\\\frac{5}{12}$.  What is the probability of the spinner landing on $C$? Express your answer as a common fraction.', 'data_source': 'MATH/PRM-800K'}\n"
     ]
    }
   ],
   "source": [
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "891ad663-25a7-49bc-9fdc-3255fe943d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['instruction']}\\n ### Answer: {example['output']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf7b8459-a415-41a5-b1ea-556776cb7a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: A board game spinner is divided into three parts labeled $A$, $B$  and $C$. The probability of the spinner landing on $A$ is $\\frac{1}{3}$ and the probability of the spinner landing on $B$ is $\\frac{5}{12}$.  What is the probability of the spinner landing on $C$? Express your answer as a common fraction.\n",
      " ### Answer: To find the probability of the spinner landing on $C$, I need to subtract the probabilities of the spinner landing on $A$ and $B$ from $1$, since the sum of the probabilities of all possible outcomes is $1$. I can write this as an equation: $P(C) = 1 - P(A) - P(B)$. I know that $P(A) = \\frac{1}{3}$ and $P(B) = \\frac{5}{12}$, so I can plug those values into the equation and simplify. I get: $P(C) = 1 - \\frac{1}{3} - \\frac{5}{12} = \\frac{12}{12} - \\frac{4}{12} - \\frac{5}{12} = \\frac{3}{12}$. I can reduce this fraction by dividing the numerator and denominator by $3$, and I get: $P(C) = \\frac{1}{4}$. \n"
     ]
    }
   ],
   "source": [
    "print(formatting_func(ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f324f9-2e7b-44e4-a585-55cf04381f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0031c2-5812-4c35-b65c-b57c938d463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da618f8-27ad-4541-9d02-b459699e67a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0d3d10a9344dba80acac4f6a8baa82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_id = \"microsoft/phi-2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True, torch_dtype=torch.float16, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b503b6f-761a-4d49-8e21-49b9d7fcd07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q wandb -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "470fa781-1246-4180-9da9-504243f40fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myinghaomaxliu06\u001b[0m (\u001b[33myinghaomaxliu06-university-of-michigan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"journal-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba9f5de7-90fd-42f9-ae98-b48db4ea4df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "    use_fast=False, # needed for now, should be fixed soon\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cd8dc3a-7058-407d-919a-775a0ebe47d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "# tokenized_val_dataset = val_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f78746ea-c5f8-4b19-9018-4bde0b5b60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "# plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c45f5eb-52cf-400e-8843-da2481dad9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2048 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c3e2539-c248-439b-90f8-ba9ec914593b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a21f9a980f411483e78f2c6a2421d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19940 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245538da48ed4758b61004def5db3290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4986 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = val_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c27a1f9e-ede3-4ce1-bb6d-65800af032ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6512a5bb-8f92-4468-8ce4-06b65466031d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24926\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "756f3ccb-2238-4b83-ab90-4e88668222d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ea8d25f-dd7d-4522-8d89-6dd0107a101b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiForCausalLM(\n",
      "  (model): PhiModel(\n",
      "    (embed_tokens): Embedding(51200, 2560)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x PhiDecoderLayer(\n",
      "        (self_attn): PhiSdpaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (k_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (v_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "          (rotary_emb): PhiRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): PhiMLP(\n",
      "          (activation_fn): NewGELUActivation()\n",
      "          (fc1): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
      "          (fc2): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): PhiRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff86b2e8-6232-4f73-a2b8-434b15186094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 26214400 || all params: 2805898240 || trainable%: 0.9342605382581515\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"Wqkv\",\n",
    "        \"fc1\",\n",
    "        \"fc2\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7777da2f-7dc7-4542-81c1-6e455fa081cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): PhiForCausalLM(\n",
      "      (model): PhiModel(\n",
      "        (embed_tokens): Embedding(51200, 2560)\n",
      "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x PhiDecoderLayer(\n",
      "            (self_attn): PhiSdpaAttention(\n",
      "              (q_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "              (k_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "              (v_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "              (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
      "              (rotary_emb): PhiRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): PhiMLP(\n",
      "              (activation_fn): NewGELUActivation()\n",
      "              (fc1): lora.Linear8bitLt(\n",
      "                (base_layer): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=10240, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (fc2): lora.Linear8bitLt(\n",
      "                (base_layer): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=10240, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (rotary_emb): PhiRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a217b52f-1eb8-42ca-9a79-6ec5aab17378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q -U datasets scipy ipywidgets matplotlib einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8be3fbb9-2479-4821-b757-e81655cfdcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19836985-8261-4e04-a73f-ef459a80ebab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 26.31 MiB is free. Process 831038 has 39.36 GiB memory in use. Of the allocated memory 38.04 GiB is allocated by PyTorch, and 836.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 35\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2112\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2110\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2113\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2458\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2457\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2458\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2461\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2462\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2463\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2464\u001b[0m ):\n\u001b[1;32m   2465\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2466\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3565\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3564\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3565\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3567\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3571\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3615\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3612\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3613\u001b[0m \u001b[38;5;66;03m# if num_items_in_batch is not None:\u001b[39;00m\n\u001b[1;32m   3614\u001b[0m \u001b[38;5;66;03m#     inputs[\"num_items_in_batch\"] = num_items_in_batch\u001b[39;00m\n\u001b[0;32m-> 3615\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3616\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3617\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1672\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1671\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1672\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1683\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/phi/modeling_phi.py:1234\u001b[0m, in \u001b[0;36mPhiForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1231\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1234\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/phi/modeling_phi.py:938\u001b[0m, in \u001b[0;36mPhiModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    931\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    932\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    933\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    935\u001b[0m         )\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 938\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_position \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    941\u001b[0m     past_seen_tokens \u001b[38;5;241m=\u001b[39m past_key_values\u001b[38;5;241m.\u001b[39mget_seq_length() \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 26.31 MiB is free. Process 831038 has 39.36 GiB memory in use. Of the allocated memory 38.04 GiB is allocated by PyTorch, and 836.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"journal-finetune\"\n",
    "base_model_name = \"phi2\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_steps=500,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=25,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff243f6a-ba41-48cd-b6c7-178819c98b55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
