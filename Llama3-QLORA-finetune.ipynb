{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e575ab-cf73-4817-b2ed-d49f4895cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"garage-bAInd/Open-Platypus\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839b1c07-2369-40ed-97b9-a5a961e32e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c877fe79-0f71-4b94-b09b-770f9c4b6811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 19940\n",
      "Validation set size: 4986\n"
     ]
    }
   ],
   "source": [
    "train_val_split = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "# Extract the training and validation datasets\n",
    "train_dataset = train_val_split['train']\n",
    "val_dataset = train_val_split['test']\n",
    "\n",
    "# Print the size of the datasets\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f39ec1a0-c645-4844-a90a-63ca75390652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['instruction']}\\n ### Answer: {example['output']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "072d7438-2463-45d0-a966-88164372d95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: A board game spinner is divided into three parts labeled $A$, $B$  and $C$. The probability of the spinner landing on $A$ is $\\frac{1}{3}$ and the probability of the spinner landing on $B$ is $\\frac{5}{12}$.  What is the probability of the spinner landing on $C$? Express your answer as a common fraction.\n",
      " ### Answer: To find the probability of the spinner landing on $C$, I need to subtract the probabilities of the spinner landing on $A$ and $B$ from $1$, since the sum of the probabilities of all possible outcomes is $1$. I can write this as an equation: $P(C) = 1 - P(A) - P(B)$. I know that $P(A) = \\frac{1}{3}$ and $P(B) = \\frac{5}{12}$, so I can plug those values into the equation and simplify. I get: $P(C) = 1 - \\frac{1}{3} - \\frac{5}{12} = \\frac{12}{12} - \\frac{4}{12} - \\frac{5}{12} = \\frac{3}{12}$. I can reduce this fraction by dividing the numerator and denominator by $3$, and I get: $P(C) = \\frac{1}{4}$. \n"
     ]
    }
   ],
   "source": [
    "print(formatting_func(ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f66eacb-57ac-491f-9a92-781df1cbea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2d925a1-826b-4770-b927-6d5d2959c0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4603c0ffcf0a427e9cda5988bf6d7b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23e7b922-4247-4a61-9884-9131c6acfb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "    use_fast=False, # needed for now, should be fixed soon\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67a62421-7e3e-43df-8f06-e1c895644579",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2048 # This was an appropriate max length for my dataset\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8538973d-1c01-48ce-93fe-fb071b0f52be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac48d56f72b4905a5ad18befca84b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19940 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7335e6f67040079f51f3992989ecc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4986 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = val_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b9c5e8e7-4327-4995-93ad-03e47665f838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24926\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad649024-998b-405c-b217-5bcb21ceb56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91dbd52b-2f0e-48db-99ba-b9b2a560d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87df72ae-24fc-45e9-91af-9d4e51cfb626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db5c1f97-a255-4d68-adb0-498b8f55040a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (1.2.0.dev0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate) (0.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate) (2.1.3)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2024.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9f5258e-5ed6-4a81-bffd-ce238fa73b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1221595-3f5c-4a0c-a215-e75f822571b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 22030336 || all params: 4562630656 || trainable%: 0.4828428523143645\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62b92005-6029-4796-ae07-0db6ea5934ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): PeftModelForCausalLM(\n",
      "      (base_model): LoraModel(\n",
      "        (model): PeftModelForCausalLM(\n",
      "          (base_model): LoraModel(\n",
      "            (model): PeftModelForCausalLM(\n",
      "              (base_model): LoraModel(\n",
      "                (model): PeftModelForCausalLM(\n",
      "                  (base_model): LoraModel(\n",
      "                    (model): LlamaForCausalLM(\n",
      "                      (model): LlamaModel(\n",
      "                        (embed_tokens): Embedding(128256, 4096)\n",
      "                        (layers): ModuleList(\n",
      "                          (0-31): 32 x LlamaDecoderLayer(\n",
      "                            (self_attn): LlamaSdpaAttention(\n",
      "                              (q_proj): lora.Linear4bit(\n",
      "                                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                                (lora_dropout): ModuleDict(\n",
      "                                  (default): Dropout(p=0.05, inplace=False)\n",
      "                                )\n",
      "                                (lora_A): ModuleDict(\n",
      "                                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                                )\n",
      "                                (lora_B): ModuleDict(\n",
      "                                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                                )\n",
      "                                (lora_embedding_A): ParameterDict()\n",
      "                                (lora_embedding_B): ParameterDict()\n",
      "                                (lora_magnitude_vector): ModuleDict()\n",
      "                              )\n",
      "                              (k_proj): lora.Linear4bit(\n",
      "                                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                                (lora_dropout): ModuleDict(\n",
      "                                  (default): Dropout(p=0.05, inplace=False)\n",
      "                                )\n",
      "                                (lora_A): ModuleDict(\n",
      "                                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                                )\n",
      "                                (lora_B): ModuleDict(\n",
      "                                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                                )\n",
      "                                (lora_embedding_A): ParameterDict()\n",
      "                                (lora_embedding_B): ParameterDict()\n",
      "                                (lora_magnitude_vector): ModuleDict()\n",
      "                              )\n",
      "                              (v_proj): lora.Linear4bit(\n",
      "                                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                                (lora_dropout): ModuleDict(\n",
      "                                  (default): Dropout(p=0.05, inplace=False)\n",
      "                                )\n",
      "                                (lora_A): ModuleDict(\n",
      "                                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                                )\n",
      "                                (lora_B): ModuleDict(\n",
      "                                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                                )\n",
      "                                (lora_embedding_A): ParameterDict()\n",
      "                                (lora_embedding_B): ParameterDict()\n",
      "                                (lora_magnitude_vector): ModuleDict()\n",
      "                              )\n",
      "                              (o_proj): lora.Linear4bit(\n",
      "                                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                                (lora_dropout): ModuleDict(\n",
      "                                  (default): Dropout(p=0.05, inplace=False)\n",
      "                                )\n",
      "                                (lora_A): ModuleDict(\n",
      "                                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                                )\n",
      "                                (lora_B): ModuleDict(\n",
      "                                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                                )\n",
      "                                (lora_embedding_A): ParameterDict()\n",
      "                                (lora_embedding_B): ParameterDict()\n",
      "                                (lora_magnitude_vector): ModuleDict()\n",
      "                              )\n",
      "                              (rotary_emb): LlamaRotaryEmbedding()\n",
      "                            )\n",
      "                            (mlp): LlamaMLP(\n",
      "                              (gate_proj): lora.Linear4bit(\n",
      "                                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                                (lora_dropout): ModuleDict(\n",
      "                                  (default): Dropout(p=0.05, inplace=False)\n",
      "                                )\n",
      "                                (lora_A): ModuleDict(\n",
      "                                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                                )\n",
      "                                (lora_B): ModuleDict(\n",
      "                                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
      "                                )\n",
      "                                (lora_embedding_A): ParameterDict()\n",
      "                                (lora_embedding_B): ParameterDict()\n",
      "                                (lora_magnitude_vector): ModuleDict()\n",
      "                              )\n",
      "                              (up_proj): lora.Linear4bit(\n",
      "                                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                                (lora_dropout): ModuleDict(\n",
      "                                  (default): Dropout(p=0.05, inplace=False)\n",
      "                                )\n",
      "                                (lora_A): ModuleDict(\n",
      "                                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                                )\n",
      "                                (lora_B): ModuleDict(\n",
      "                                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
      "                                )\n",
      "                                (lora_embedding_A): ParameterDict()\n",
      "                                (lora_embedding_B): ParameterDict()\n",
      "                                (lora_magnitude_vector): ModuleDict()\n",
      "                              )\n",
      "                              (down_proj): lora.Linear4bit(\n",
      "                                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                                (lora_dropout): ModuleDict(\n",
      "                                  (default): Dropout(p=0.05, inplace=False)\n",
      "                                )\n",
      "                                (lora_A): ModuleDict(\n",
      "                                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
      "                                )\n",
      "                                (lora_B): ModuleDict(\n",
      "                                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                                )\n",
      "                                (lora_embedding_A): ParameterDict()\n",
      "                                (lora_embedding_B): ParameterDict()\n",
      "                                (lora_magnitude_vector): ModuleDict()\n",
      "                              )\n",
      "                              (act_fn): SiLU()\n",
      "                            )\n",
      "                            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "                            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "                          )\n",
      "                        )\n",
      "                        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "                        (rotary_emb): LlamaRotaryEmbedding()\n",
      "                      )\n",
      "                      (lm_head): lora.Linear(\n",
      "                        (base_layer): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "                        (lora_dropout): ModuleDict(\n",
      "                          (default): Dropout(p=0.05, inplace=False)\n",
      "                        )\n",
      "                        (lora_A): ModuleDict(\n",
      "                          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                        )\n",
      "                        (lora_B): ModuleDict(\n",
      "                          (default): Linear(in_features=8, out_features=128256, bias=False)\n",
      "                        )\n",
      "                        (lora_embedding_A): ParameterDict()\n",
      "                        (lora_embedding_B): ParameterDict()\n",
      "                        (lora_magnitude_vector): ModuleDict()\n",
      "                      )\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c559f449-d619-415c-b3b3-fb7dcc54e6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myinghal\u001b[0m (\u001b[33myinghal-university-of-michigan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"llama-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "514a9ed3-2bcf-4c20-bdae-f0578c71c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71a5af-c0f0-4754-bae0-1fc99df966b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  51/1000 07:58 < 2:34:36, 0.10 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='208' max='624' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [208/624 08:31 < 17:07, 0.40 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"llama-finetune\"\n",
    "base_model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5,\n",
    "        logging_steps=50,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=25,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf435369-9495-4fc5-a202-0a1373772e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
